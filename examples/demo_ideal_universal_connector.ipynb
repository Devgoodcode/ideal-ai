{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß© Ideal AI - Universal LLM Connector Demo\n",
    "\n",
    "> **One Connector to Rule Them All**\n",
    "\n",
    "This notebook demonstrates the **ideal-ai** package - a professional, pip-installable universal LLM connector.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. ‚úÖ Modern package installation\n",
    "2. ‚úÖ Text generation across multiple providers\n",
    "3. ‚úÖ Vision/multimodal capabilities\n",
    "4. ‚úÖ Image generation\n",
    "5. ‚úÖ Audio transcription\n",
    "6. ‚úÖ Speech synthesis (TTS)\n",
    "7. ‚úÖ Video generation (async)\n",
    "8. ‚úÖ Runtime model injection\n",
    "9. ‚úÖ Smolagents integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation\n",
    "\n",
    "Install the package (skip if already installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install from PyPI (when published)\n",
    "%pip install ideal-ai\n",
    "\n",
    "# Or install from local source (development mode)\n",
    "#%pip install -e .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîë Setup COLAB API Keys\n",
    "\n",
    "Load your API keys from environment or .env file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"üöÄ Ideal-AI Demo Configuration\")\n",
    "print(\"‚ÑπÔ∏è The script will attempt to load keys from Colab Secrets.\")\n",
    "print(\"‚ÑπÔ∏è If a permission popup appears and you click 'No' or 'Cancel', the key will be skipped automatically.\\n\")\n",
    "\n",
    "# 1. SETUP: Helper function to load silently\n",
    "def load_key_silently(env_var_name, secret_name, display_name):\n",
    "    \"\"\"Try to load from secrets. If fails, just skip. No blocking input.\"\"\"\n",
    "    try:\n",
    "        from google.colab import userdata\n",
    "        val = userdata.get(secret_name)\n",
    "        if val:\n",
    "            os.environ[env_var_name] = val\n",
    "            print(f\"‚úÖ {display_name}: Loaded via Secrets.\")\n",
    "            return True\n",
    "    except Exception:\n",
    "        # This catches \"SecretNotFoundError\" AND \"NotebookAccessError\" (User said No)\n",
    "        # Crucial: We do NOT call getpass() here. We just pass.\n",
    "        pass\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è {display_name}: Skipped (Not found in Secrets or denied).\")\n",
    "    return False\n",
    "\n",
    "# --- 2. EXECUTION: Loop through keys non-stop ---\n",
    "\n",
    "# A. CORE KEYS\n",
    "print(\"üîπ STEP 1: The Essentials\")\n",
    "load_key_silently(\"GOOGLE_API_KEY\", \"GOOGLE_API_KEY\", \"Google (Gemini)\")\n",
    "load_key_silently(\"OPENAI_API_KEY\", \"OPENAI_API_KEY\", \"OpenAI\")\n",
    "load_key_silently(\"ALIBABA_API_KEY\", \"ALIBABA_API_KEY\", \"Alibaba (Wan 2.1)\")\n",
    "\n",
    "# B. OPTIONAL KEYS\n",
    "print(\"\\nüîπ STEP 2: Bonus Keys\")\n",
    "OPTIONAL_KEYS = [\n",
    "    (\"INFOMANIAK_AI_TOKEN\", \"Infomaniak Token\"),\n",
    "    (\"INFOMANIAK_PRODUCT_ID\", \"Infomaniak Product ID\"),\n",
    "    (\"ANTHROPIC_API_KEY\", \"Anthropic\"),\n",
    "    (\"MOONSHOT_API_KEY\", \"Moonshot\"),\n",
    "    (\"PERPLEXITY_API_KEY\", \"Perplexity\"),\n",
    "    (\"HF_TOKEN\", \"Hugging Face\"),\n",
    "    (\"MINIMAX_API_KEY\", \"MiniMax\")\n",
    "]\n",
    "\n",
    "count = 0\n",
    "for env_key, label in OPTIONAL_KEYS:\n",
    "    # Use the same secret name as env var for simplicity\n",
    "    if load_key_silently(env_key, env_key, label):\n",
    "        count += 1\n",
    "\n",
    "print(f\"\\n‚úÖ Configuration complete. {count} bonus keys loaded.\")\n",
    "\n",
    "# --- 3. OLLAMA (Local) ---\n",
    "os.environ[\"OLLAMA_URL\"] = \"http://localhost:11434\"\n",
    "\n",
    "# --- 4. MANUAL FALLBACK (Optional) ---\n",
    "# Only ask ONCE if the user really wants to type manually\n",
    "print(\"\\nüí° Tip: If you don't use Colab Secrets, you can set keys manually below.\")\n",
    "# Uncomment the lines below if you want to force manual input:\n",
    "# if not os.getenv(\"OPENAI_API_KEY\") and not os.getenv(\"GOOGLE_API_KEY\"):\n",
    "#     k = getpass(\"üëâ Enter OpenAI or Google Key manually (or press Enter to skip): \")\n",
    "#     if k.strip(): os.environ[\"OPENAI_API_KEY\"] = k.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Initialize Connector\n",
    "\n",
    "**NEW**: No more `sys.path.append` hacks! Just import the package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern import - works after pip install\n",
    "from ideal_ai import IdealUniversalLLMConnector\n",
    "import os\n",
    "\n",
    "# --- FIX: Reconstruct dictionary from Environment Variables ---\n",
    "API_KEYS = {\n",
    "    \"openai\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "    \"google\": os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    \"anthropic\": os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "    \"alibaba\": os.getenv(\"ALIBABA_API_KEY\"),\n",
    "    \"infomaniak\": os.getenv(\"INFOMANIAK_AI_TOKEN\"),\n",
    "    \"infomaniak_product\": os.getenv(\"INFOMANIAK_PRODUCT_ID\"),\n",
    "    \"moonshot\": os.getenv(\"MOONSHOT_API_KEY\"),\n",
    "    \"perplexity\": os.getenv(\"PERPLEXITY_API_KEY\"),\n",
    "    \"hugging_face\": os.getenv(\"HF_TOKEN\"),\n",
    "    \"minimax\": os.getenv(\"MINIMAX_API_KEY\"),\n",
    "}\n",
    "\n",
    "# Initialize connector\n",
    "connector = IdealUniversalLLMConnector(\n",
    "    api_keys=API_KEYS,\n",
    "    ollama_url=os.getenv(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Connector initialized successfully!\")\n",
    "print(f\"ü§ñ Ready to use with {len(connector.model_configs)} pre-configured models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã List Available Models\n",
    "\n",
    "See what's available out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "available = connector.list_available_families(include_models=True)\n",
    "\n",
    "print(\"\\n=== Available Models by Modality ===\")\n",
    "for modality, families in available.items():\n",
    "    print(f\"\\nüéØ {modality.upper()}:\")\n",
    "    for family_name, details in families.items():\n",
    "        models = details.get(\"models\", [])\n",
    "        if isinstance(models, list) and models:\n",
    "            # Extract model names from list\n",
    "            model_names = [m.get(\"model\") if isinstance(m, dict) else m for m in models[:3]]\n",
    "            print(f\"  ‚Ä¢ {family_name}: {', '.join(model_names)}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Text Generation: The \"Universal Loop\" ‚ôæÔ∏è\n",
    "\n",
    "This is the core promise of **ideal-ai**: One unified interface for all providers.\n",
    "\n",
    "Instead of writing specific code for OpenAI, Google, or Alibaba, we iterate through them\n",
    "using **the exact same code structure**.\n",
    "\n",
    "*Note: The loop below automatically detects which API keys you provided in the setup step and only runs on available providers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- THE SMART LOOP CONFIGURATION ---\n",
    "# We define potential targets. The code checks environment variables loaded\n",
    "# in the \"Setup\" step. It ONLY runs providers you actually configured.\n",
    "# NO extra API keys will be requested here.\n",
    "potential_targets = [\n",
    "    (\"openai\", \"gpt-4o\", [\"OPENAI_API_KEY\"]),\n",
    "    (\"google\", \"gemini-2.5-flash\", [\"GOOGLE_API_KEY\"]),\n",
    "    (\"anthropic\", \"claude-haiku-4-5-20251001\", [\"ANTHROPIC_API_KEY\"]),\n",
    "    (\"alibaba\", \"qwen-plus\", [\"ALIBABA_API_KEY\"]),\n",
    "    (\"infomaniak\", \"mixtral\", [\"INFOMANIAK_AI_TOKEN\", \"INFOMANIAK_PRODUCT_ID\"]),\n",
    "    (\"mistral\", \"mistral-large-latest\", [\"MISTRAL_API_KEY\"])\n",
    "]\n",
    "\n",
    "# Build the active list dynamically based on loaded keys\n",
    "active_targets = []\n",
    "for provider, model, required_keys in potential_targets:\n",
    "    if all(os.getenv(k) for k in required_keys):\n",
    "        active_targets.append({\"provider\": provider, \"model\": model})\n",
    "\n",
    "# üí° MARKETING PROMPT: We ask the AI to explain why THIS tool is useful!\n",
    "prompt = \"Explain the benefit of having one Universal API for all AI providers in one funny sentence.\"\n",
    "\n",
    "print(f\"üé§ Prompt: \\\"{prompt}\\\"\\n\")\n",
    "\n",
    "if not active_targets:\n",
    "    print(\"‚ö†Ô∏è No API keys detected! Please run the 'Setup Keys' cell above.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Detected {len(active_targets)} active providers. Running the Universal Loop...\\n\")\n",
    "\n",
    "    # --- THE MAGIC: ONE CODE BLOCK FOR EVERYONE ---\n",
    "    for target in active_targets:\n",
    "        print(f\"‚è≥ Calling {target['provider'].upper()} ({target['model']})...\")\n",
    "        try:\n",
    "            response = connector.invoke(\n",
    "                provider=target[\"provider\"],\n",
    "                model_id=target[\"model\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            print(f\"   ü§ñ Result: {response['text']}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error: {e}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with cloud providers\n",
    "providers_to_test = [\n",
    "    (\"openai\", \"gpt-4o\"),\n",
    "    (\"google\", \"gemini-2.5-flash\"),\n",
    "    (\"anthropic\", \"claude-haiku-4-5-20251001\"),\n",
    "]\n",
    "\n",
    "prompt = \"What is the meaning of life? Answer in max 50 characters.\"\n",
    "\n",
    "for provider, model in providers_to_test:\n",
    "    try:\n",
    "        result = connector.invoke(\n",
    "            provider=provider,\n",
    "            model_id=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        print(f\"\\n‚úÖ {provider}/{model}:\")\n",
    "        print(f\"   {result['text']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå {provider}/{model}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Vision/Multimodal\n",
    "\n",
    "Analyze images with vision models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "\n",
    "# Download test image from ia-agence.ai\n",
    "image_url = \"https://ia-agence.ai/wp-content/uploads/2025/12/demo_image_ideal_ai_connector.jpg\"\n",
    "response = requests.get(image_url)\n",
    "image_bytes = response.content\n",
    "\n",
    "# Display image\n",
    "display(Image.open(BytesIO(image_bytes)))\n",
    "\n",
    "# Analyze with vision model\n",
    "analysis = connector.invoke_image(\n",
    "    provider=\"google\",  # or \"openai\", \"anthropic\", \"ollama\"\n",
    "    model_id=\"gemini-2.5-flash\",\n",
    "    image_input=image_bytes,\n",
    "    prompt=\"Describe what you see in this image in English.\"\n",
    ")\n",
    "\n",
    "print(\"\\nüëÅÔ∏è Vision Analysis:\")\n",
    "print(analysis[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Image Generation\n",
    "\n",
    "Generate images from text prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import Image as DisplayImage\n",
    "\n",
    "# Generate image\n",
    "result = connector.invoke_image_generation(\n",
    "    provider=\"openai\",  # or \"infomaniak\"\n",
    "    model_id=\"dall-e-3\",\n",
    "    prompt=\"A cute robot sitting on a rock, looking at a castle, warm sunset light, Disney style\",\n",
    "    width=1024,\n",
    "    height=1024\n",
    ")\n",
    "\n",
    "if result.get(\"images\"):\n",
    "    # Decode and display\n",
    "    b64_string = result[\"images\"][0]\n",
    "    img_bytes = base64.b64decode(b64_string)\n",
    "    \n",
    "    print(\"üé® Generated Image:\")\n",
    "    display(DisplayImage(data=img_bytes))\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = \"generated_image.png\"\n",
    "    Image.open(BytesIO(img_bytes)).save(output_path)\n",
    "    print(f\"‚úÖ Saved to: {output_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No image generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Audio Transcription\n",
    "\n",
    "Convert speech to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Define file paths\n",
    "local_file = \"demo_audio.m4a\"\n",
    "remote_url = \"https://ia-agence.ai/wp-content/uploads/2025/12/demo_sound_ideal_ai_connector.m4a\"\n",
    "\n",
    "# Download demo file automatically\n",
    "if not Path(local_file).exists():\n",
    "    with open(local_file, \"wb\") as f:\n",
    "        f.write(requests.get(remote_url).content)\n",
    "\n",
    "# Run transcription\n",
    "transcription = connector.invoke_audio(\n",
    "    provider=\"infomaniak\",\n",
    "    model_id=\"whisper\",\n",
    "    audio_file_path=local_file,\n",
    "    language=\"en\"\n",
    ")\n",
    "\n",
    "print(\"üé§ Transcription:\")\n",
    "print(transcription[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Speech Synthesis (TTS)\n",
    "\n",
    "Convert text to speech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Generate speech\n",
    "audio_result = connector.invoke_speech_generation(\n",
    "    provider=\"openai\",\n",
    "    model_id=\"tts-1\",\n",
    "    text=\"Hello! This is a test of the text-to-speech system using the Ideal AI connector.\",\n",
    "    voice=\"nova\"  # Available: alloy, echo, fable, onyx, nova, shimmer\n",
    ")\n",
    "\n",
    "if audio_result.get(\"audio_bytes\"):\n",
    "    print(\"üó£Ô∏è Generated Speech:\")\n",
    "    display(Audio(data=audio_result[\"audio_bytes\"]))\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"generated_speech.mp3\", \"wb\") as f:\n",
    "        f.write(audio_result[\"audio_bytes\"])\n",
    "    print(\"‚úÖ Saved to: generated_speech.mp3\")\n",
    "else:\n",
    "    print(\"‚ùå No audio generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Video Generation\n",
    "\n",
    "Generate video from text (async task with automatic polling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "print(\"üé¨ Starting video generation...\")\n",
    "print(\"‚è≥ This may take 1-5 minutes (polling handled automatically)\")\n",
    "\n",
    "video_result = connector.invoke_video_generation(\n",
    "    provider=\"alibaba\",\n",
    "    model_id=\"wan2.1-t2v-turbo\",  # or \"wan2.2-t2v-plus\", \"wan2.5-t2v-preview\"\n",
    "    prompt=\"A futuristic robot standing in a high-tech urban environment at night\",\n",
    "    size=\"832*480\"  # or \"1280*720\"\n",
    ")\n",
    "\n",
    "if video_result.get(\"videos\"):\n",
    "    video_url = video_result[\"videos\"][0]\n",
    "    print(f\"\\nüéâ Video generated!\")\n",
    "    print(f\"üìπ URL: {video_url}\")\n",
    "    \n",
    "    # Download and display\n",
    "    video_bytes = requests.get(video_url, timeout=60).content\n",
    "    with open(\"generated_video.mp4\", \"wb\") as f:\n",
    "        f.write(video_bytes)\n",
    "    \n",
    "    print(\"‚úÖ Saved to: generated_video.mp4\")\n",
    "    display(Video(\"generated_video.mp4\", embed=True, width=400))\n",
    "else:\n",
    "    print(\"‚ùå No video generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Runtime Model Injection\n",
    "\n",
    "**NEW FEATURE**: Add custom models without modifying source code!\n",
    "\n",
    "This demonstrates the maintenance system - you can now add models just by editing JSON or injecting at runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Add a custom Ollama model at runtime\n",
    "connector.register_model(\n",
    "    \"ollama:custom-model\",\n",
    "    {\n",
    "        \"families\": {\n",
    "            \"text\": \"ollama_text\"  # Reuse existing recipe\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom model registered!\")\n",
    "\n",
    "# Example: Add a cloud provider model\n",
    "connector.register_model(\n",
    "    \"huggingface:custom-gpt\",\n",
    "    {\n",
    "        \"api_key_name\": \"hugging_face\",\n",
    "        \"families\": {\n",
    "            \"text\": \"openai_compatible\"\n",
    "        },\n",
    "        \"url_template\": \"https://router.huggingface.co/v1/chat/completions\",\n",
    "        \"api_model_name\": \"openai/gpt-oss-20b\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Cloud provider model registered!\")\n",
    "print(\"\\nüí° These models can now be used immediately with connector.invoke()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Multi-Turn Conversation\n",
    "\n",
    "Build conversational agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conversation history\n",
    "conversation = []\n",
    "\n",
    "def chat(user_message: str, provider: str = \"openai\", model: str = \"gpt-4o\"):\n",
    "    \"\"\"Send a message and get AI response.\"\"\"\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_message})\n",
    "    \n",
    "    response = connector.invoke(\n",
    "        provider=provider,\n",
    "        model_id=model,\n",
    "        messages=conversation,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    ai_message = response[\"text\"]\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": ai_message})\n",
    "    \n",
    "    print(f\"ü§ñ AI: {ai_message}\")\n",
    "    return ai_message\n",
    "\n",
    "# Example conversation\n",
    "print(\"üë§ User: Hello! What's your name?\")\n",
    "chat(\"Hello! What's your name?\")\n",
    "\n",
    "print(\"\\nüë§ User: Can you help me with Python?\")\n",
    "chat(\"Can you help me with Python?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Smolagents Integration\n",
    "\n",
    "Use with smolagents for building AI agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ideal_ai import IdealSmolagentsWrapper\n",
    "from smolagents import CodeAgent, tool\n",
    "\n",
    "# 1. Define a custom tool (Robust & Dependency-free!)\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the current weather for a specific location.\n",
    "    Args:\n",
    "        location: The name of the city (e.g. 'Paris', 'New York')\n",
    "    \"\"\"\n",
    "    # This is a mock for the demo - zero failure risk\n",
    "    return f\"The weather in {location} is sunny with a perfect temperature for coding!\"\n",
    "\n",
    "# 2. Wrap connector for smolagents\n",
    "model = IdealSmolagentsWrapper(\n",
    "    connector=connector,\n",
    "    provider=\"openai\", # Ensure you have a key set for this provider\n",
    "    model_id=\"gpt-4o\"\n",
    ")\n",
    "\n",
    "# 3. Create agent with our custom tool\n",
    "agent = CodeAgent(\n",
    "    tools=[get_weather],\n",
    "    model=model\n",
    ")\n",
    "\n",
    "# 4. Run agent task\n",
    "print(\"ü§ñ Agent working...\")\n",
    "result = agent.run(\"What is the weather in Paris? And tell me a joke about it.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Advanced: Custom Parser\n",
    "\n",
    "Handle non-standard API response formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom parser for a specific provider\n",
    "def my_custom_parser(raw_response):\n",
    "    \"\"\"Extract text from custom API response format.\"\"\"\n",
    "    # Example: handle nested response\n",
    "    return raw_response.get(\"data\", {}).get(\"output\", {}).get(\"text\", \"\")\n",
    "\n",
    "# Create connector with custom parser\n",
    "custom_connector = IdealUniversalLLMConnector(\n",
    "    api_keys=API_KEYS,\n",
    "    parsers={\n",
    "        \"provider:model-id\": my_custom_parser  # Specific model\n",
    "        # or \"provider\": my_custom_parser  # All models from provider\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Connector created with custom parser\")\n",
    "print(\"üí° The custom parser will be used automatically when calling that model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üêõ Debugging\n",
    "\n",
    "Enable debug mode to inspect API calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug to see raw payloads and responses\n",
    "try:\n",
    "    debug_response = connector.invoke(\n",
    "        provider=\"google\",\n",
    "        model_id=\"gemini-2.5-flash\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hi! System check.\"}],\n",
    "        debug=True  # ‚Üê Enable debug mode to see the JSON payloads\n",
    "    )\n",
    "\n",
    "    print(\"\\nüìä Debug output shown above\")\n",
    "    print(f\"üìù Response: {debug_response['text']}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Debug test failed: {e}\")\n",
    "    print(\"Tip: Check if your GOOGLE_API_KEY is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Interactive Testing Interface\n",
    "\n",
    "**Bonus**: All-in-one graphical testing tool for quick experimentation\n",
    "\n",
    "This section provides a comprehensive widget-based interface to test all modalities without writing code. Perfect for quick prototyping and model comparison.\n",
    "\n",
    "**Features:**\n",
    "- ‚úçÔ∏è Text generation (one-shot + multi-turn chat)\n",
    "- üéØ Multi-model benchmark comparison\n",
    "- üëÅÔ∏è Vision/image analysis\n",
    "- üé® Image generation\n",
    "- üé¨ Video generation\n",
    "- üó£Ô∏è Speech synthesis (TTS)\n",
    "- üé§ Audio transcription (STT)\n",
    "- üó£Ô∏èüéôÔ∏è Voice chat (full audio-to-audio pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required widget libraries\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, AppLayout \n",
    "from IPython.display import display, Markdown, clear_output, Image as DisplayImage, Audio, Video, HTML\n",
    "import sys\n",
    "import base64\n",
    "import requests \n",
    "import io  \n",
    "import os \n",
    "import json\n",
    "from pathlib import Path\n",
    "from tabulate import tabulate \n",
    "from datetime import datetime\n",
    "\n",
    "# Audio-specific imports for voice chat\n",
    "try:\n",
    "    import sounddevice as sd\n",
    "    import numpy as np\n",
    "    from scipy.io.wavfile import write\n",
    "    import pygame\n",
    "    import time\n",
    "    AUDIO_LIBS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    AUDIO_LIBS_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è Audio libraries not available. Voice chat will be disabled.\")\n",
    "\n",
    "# PIL for image handling\n",
    "try:\n",
    "    from PIL import Image as PILImage\n",
    "except ImportError:\n",
    "    PILImage = None\n",
    "\n",
    "# Verify connector is initialized\n",
    "if 'connector' not in locals() or not connector:\n",
    "    print(\"‚ùå ERROR: Connector not initialized. Please run the initialization cell first.\")\n",
    "else:\n",
    "    print(\"‚úÖ Building unified testing interface...\")\n",
    "    \n",
    "    # Initialize pygame mixer for audio playback\n",
    "    if AUDIO_LIBS_AVAILABLE:\n",
    "        try:\n",
    "            pygame.mixer.init()\n",
    "            print(\"üéß Pygame mixer initialized for audio playback.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Warning: Failed to initialize pygame.mixer: {e}\")\n",
    "\n",
    "    # Setup output directories for generated content\n",
    "    OUTPUT_DIRS = {}\n",
    "    try:\n",
    "        home_dir = Path.home()\n",
    "        base_output = home_dir / \"JupyterLab\" / \"notebooks\" / \"data\"\n",
    "        OUTPUT_DIRS[\"image\"] = base_output / \"images_gen_output\"\n",
    "        OUTPUT_DIRS[\"video\"] = base_output / \"videos_gen_output\"\n",
    "        OUTPUT_DIRS[\"speech\"] = base_output / \"audios_gen_output\"\n",
    "        OUTPUT_DIRS[\"audio_in\"] = base_output / \"audios_input\"\n",
    "        for path in OUTPUT_DIRS.values():\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        print(f\"üìÇ Output directories configured in {base_output}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating output directories: {e}\")\n",
    "\n",
    "    # Retrieve models by modality from connector\n",
    "    MODELS_BY_MODALITY = {\n",
    "        \"text\": [], \"vision\": [], \"image_gen\": [], \n",
    "        \"video_gen\": [], \"speech_gen\": [], \"audio\": [] \n",
    "    }\n",
    "    try:\n",
    "        available_models = connector.list_available_families(include_models=True)\n",
    "        for modality, families in available_models.items():\n",
    "            if modality in MODELS_BY_MODALITY: \n",
    "                for family_name, details in families.items():\n",
    "                    for model_item in details.get(\"models\", []):\n",
    "                        model_pair = model_item.get(\"model\") if isinstance(model_item, dict) else model_item\n",
    "                        if model_pair:\n",
    "                            MODELS_BY_MODALITY[modality].append(model_pair)\n",
    "        for mod in MODELS_BY_MODALITY:\n",
    "            MODELS_BY_MODALITY[mod] = sorted(list(set(MODELS_BY_MODALITY[mod])))\n",
    "        print(f\"‚úÖ Models loaded: {sum(len(v) for v in MODELS_BY_MODALITY.values())} total\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error retrieving model lists: {e}\")\n",
    "        # Fallback lists\n",
    "        MODELS_BY_MODALITY[\"text\"] = ['ollama:llama3.2', 'openai:gpt-4o', 'google:gemini-2.5-flash']\n",
    "        MODELS_BY_MODALITY[\"vision\"] = ['ollama:llava', 'google:gemini-2.5-flash', 'openai:gpt-4o']\n",
    "        MODELS_BY_MODALITY[\"image_gen\"] = ['infomaniak:sdxl-lightning', 'openai:dall-e-3']\n",
    "        MODELS_BY_MODALITY[\"video_gen\"] = ['alibaba:wan2.1-t2v-turbo']\n",
    "        MODELS_BY_MODALITY[\"speech_gen\"] = ['openai:tts-1']\n",
    "        MODELS_BY_MODALITY[\"audio\"] = ['infomaniak:whisper'] \n",
    "\n",
    "    # Modality icons for accordion\n",
    "    MODALITY_ICONS = {\n",
    "        \"text\": \"‚úçÔ∏è\", \"vision\": \"üëÅÔ∏è\", \"audio\": \"üîä\", \n",
    "        \"image_gen\": \"üé®\", \"video_gen\": \"üé¨\", \"speech_gen\": \"üó£Ô∏è\", \"other\": \"‚öôÔ∏è\"\n",
    "    }\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 1: TEXT TESTER (One-Shot) + ACCORDION\n",
    "    # =================================================================\n",
    "    \n",
    "    text_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"text\"], \n",
    "        value=MODELS_BY_MODALITY[\"text\"][0], \n",
    "        description='Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    text_prompt_input = widgets.Textarea(\n",
    "        value=\"Write a short poem about the beauty of code.\", \n",
    "        description='Prompt:', \n",
    "        style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='90%', height='100px')\n",
    "    )\n",
    "    text_invoke_button = widgets.Button(\n",
    "        description='üöÄ Invoke', \n",
    "        button_style='success', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    text_output_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "    \n",
    "    def on_invoke_text_clicked(b):\n",
    "        with text_output_area:\n",
    "            clear_output(wait=True)\n",
    "            selected_pair = text_llm_selector.value\n",
    "            try: \n",
    "                provider, model_id = selected_pair.split(\":\", 1)\n",
    "            except ValueError: \n",
    "                print(f\"‚ùå Format error: {selected_pair}\")\n",
    "                return\n",
    "            print(f\"--- Testing {model_id} via {provider} ---\")\n",
    "            messages = [{\"role\": \"user\", \"content\": text_prompt_input.value}]\n",
    "            try:\n",
    "                response = connector.invoke(\n",
    "                    provider=provider, \n",
    "                    model_id=model_id, \n",
    "                    messages=messages, \n",
    "                    temperature=1.0, \n",
    "                    debug=False\n",
    "                )\n",
    "                result_text = response.get(\"text\", \"No text returned.\")\n",
    "                display(Markdown(f\"### ‚úÖ Response from **{model_id}**\\n---\\n{result_text}\"))\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"### ‚ùå Error from {model_id} ({provider})\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "    \n",
    "    text_invoke_button.on_click(on_invoke_text_clicked)\n",
    "    \n",
    "    # Build accordion showing available families and models\n",
    "    try:\n",
    "        families_for_accordion = connector.list_available_families(include_models=True)\n",
    "        accordion_children = []\n",
    "        for modality, families in families_for_accordion.items():\n",
    "            html_content = \"\"\n",
    "            if not families: \n",
    "                html_content = \"<i>No families defined.</i>\"\n",
    "            else:\n",
    "                for family_name, details in families.items():\n",
    "                    html_content += f\"<h4 style='margin-bottom: 5px;'>‚ñ∫ Family: <code>{family_name}</code></h4>\"\n",
    "                    url_template = details.get(\"url_template\", \"[N/A]\")\n",
    "                    html_content += \"<ul>\"\n",
    "                    html_content += f\"<li><strong>URL Template</strong>: <code>{url_template}</code></li>\"\n",
    "                    models = details.get(\"models\", [])\n",
    "                    if not models: \n",
    "                        html_content += \"<li><strong>Models</strong>: (None)</li>\"\n",
    "                    else:\n",
    "                        html_content += \"<li><strong>Models</strong>:</li><ul style='margin-top: 5px;'>\"\n",
    "                        for model_item in models:\n",
    "                            if isinstance(model_item, dict):\n",
    "                                model_name = model_item.get(\"model\", \"Unknown\")\n",
    "                                model_url = model_item.get(\"url_template\", \"\")\n",
    "                                if \"See URLs\" in url_template: \n",
    "                                    html_content += f\"<li><code>{model_name}</code> (URL: <code>{model_url}</code>)</li>\"\n",
    "                                else: \n",
    "                                    html_content += f\"<li><code>{model_name}</code></li>\"\n",
    "                            else: \n",
    "                                html_content += f\"<li><code>{model_item}</code></li>\"\n",
    "                        html_content += \"</ul>\"\n",
    "                    html_content += \"</ul><hr style='border: none; border-top: 1px dashed #ccc; margin: 10px 0;'>\"\n",
    "            accordion_children.append(widgets.HTML(value=html_content))\n",
    "        accordion_widget = widgets.Accordion(children=accordion_children, selected_index=None)\n",
    "        for i, modality in enumerate(families_for_accordion.keys()):\n",
    "            icon = MODALITY_ICONS.get(modality, \"‚öôÔ∏è\")\n",
    "            accordion_widget.set_title(i, f\"{icon} Modality: {modality.upper()}\")\n",
    "        accordion_title = widgets.HTML(\"<h3>üß© Available Families and Models</h3>\")\n",
    "    except Exception as e:\n",
    "        accordion_widget = widgets.HTML(f\"<b>Accordion Error:</b> {e}\")\n",
    "        accordion_title = widgets.HTML(\"\")\n",
    "    \n",
    "    tab_text = widgets.VBox([\n",
    "        text_llm_selector, text_prompt_input, text_invoke_button, \n",
    "        text_output_area, widgets.HTML(\"<hr>\"), accordion_title, accordion_widget\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 2: CHATBOT (Multi-Turn Conversation)\n",
    "    # =================================================================\n",
    "    \n",
    "    chat_history = []\n",
    "    \n",
    "    chat_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"text\"], \n",
    "        value=MODELS_BY_MODALITY[\"text\"][0], \n",
    "        description='Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='100%')\n",
    "    )\n",
    "    chat_output_area = widgets.Output(\n",
    "        layout=Layout(width='100%', height='400px', overflow_y='auto', \n",
    "                     border='1px solid #ccc', padding='10px')\n",
    "    )\n",
    "    chat_output_area.add_class(\"chat-output\") \n",
    "    chat_prompt_input = widgets.Text(\n",
    "        value='', \n",
    "        description='You:', \n",
    "        placeholder='Enter your message and press [Enter]...', \n",
    "        style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='100%'),\n",
    "        continuous_update=False\n",
    "    )\n",
    "    chat_send_button = widgets.Button(description='Send', button_style='primary')\n",
    "    chat_clear_button = widgets.Button(description='Clear', button_style='danger')\n",
    "\n",
    "    def send_chat_message(source_widget):\n",
    "        provider, model_id = chat_llm_selector.value.split(\":\", 1)\n",
    "        prompt = chat_prompt_input.value\n",
    "        if not prompt:\n",
    "            return\n",
    "        chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        with chat_output_area:\n",
    "            display(HTML(f\"<p style='margin: 0;'><b>You:</b> {prompt}</p>\"))\n",
    "        chat_prompt_input.value = ''\n",
    "        messages_to_send = chat_history.copy()\n",
    "        if len(messages_to_send) == 1: \n",
    "            messages_to_send.insert(0, {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": \"You are a helpful assistant. Keep responses concise (max 150 characters).\"\n",
    "            })\n",
    "        with chat_output_area:\n",
    "            print(f\"--- {model_id} thinking... ---\")\n",
    "            try:\n",
    "                response = connector.invoke(\n",
    "                    provider=provider, model_id=model_id, \n",
    "                    messages=messages_to_send, temperature=0.7, debug=False\n",
    "                )\n",
    "                result_text = response.get(\"text\", \"Sorry, I couldn't respond.\")\n",
    "                chat_history.append({\"role\": \"assistant\", \"content\": result_text})\n",
    "                clear_output(wait=True) \n",
    "                for msg in chat_history:\n",
    "                    if msg['role'] == 'user':\n",
    "                        display(HTML(f\"<p style='margin: 0;'><b>You:</b> {msg['content']}</p>\"))\n",
    "                    else:\n",
    "                        display(HTML(\n",
    "                            f\"<p style='margin: 10px 0;'><b>{model_id}:</b> {msg['content']}</p>\"\n",
    "                            f\"<hr style='border: none; border-top: 1px dashed #ccc; margin: 10px 0;'>\"\n",
    "                        ))\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"### ‚ùå Error from {model_id} ({provider})\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "\n",
    "    def on_chat_clear_clicked(b):\n",
    "        chat_history.clear()\n",
    "        with chat_output_area:\n",
    "            clear_output()\n",
    "            print(\"Conversation history cleared.\")\n",
    "\n",
    "    chat_send_button.on_click(lambda b: send_chat_message(b))\n",
    "    chat_clear_button.on_click(on_chat_clear_clicked)\n",
    "    chat_prompt_input.observe(lambda change: send_chat_message(change), names='value')\n",
    "    \n",
    "    chat_input_box = widgets.HBox(\n",
    "        [chat_prompt_input, chat_send_button, chat_clear_button], \n",
    "        layout=Layout(width='100%', padding='5px 0 0 0')\n",
    "    )\n",
    "    tab_chat = AppLayout(\n",
    "        header=chat_llm_selector, center=chat_output_area, footer=chat_input_box,\n",
    "        pane_heights=['50px', '400px', '50px'] \n",
    "    )\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 3: BENCHMARK (Multi-Model Comparison)\n",
    "    # =================================================================\n",
    "    \n",
    "    benchmark_model_selector = widgets.SelectMultiple(\n",
    "        options=MODELS_BY_MODALITY[\"text\"], \n",
    "        value=MODELS_BY_MODALITY[\"text\"][:3], \n",
    "        description='Models to Test:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='90%', height='200px')\n",
    "    )\n",
    "    benchmark_prompt_input = widgets.Textarea(\n",
    "        value=\"What is the meaning of life? Answer in max 100 characters.\", \n",
    "        description='Prompt:', style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='90%', height='100px')\n",
    "    )\n",
    "    benchmark_invoke_button = widgets.Button(\n",
    "        description='üöÄ Run Benchmark', button_style='danger', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    benchmark_output_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "    \n",
    "    def on_invoke_benchmark_clicked(b):\n",
    "        with benchmark_output_area:\n",
    "            clear_output(wait=True)\n",
    "            models_to_test = benchmark_model_selector.value\n",
    "            if not models_to_test: \n",
    "                print(\"‚ùå Please select at least one model.\")\n",
    "                return\n",
    "            question = benchmark_prompt_input.value\n",
    "            messages = [{\"role\": \"user\", \"content\": question}]\n",
    "            results = []\n",
    "            print(f\"üìä Running benchmark for {len(models_to_test)} models...\")\n",
    "            for pair in models_to_test:\n",
    "                try: \n",
    "                    provider, model_id = pair.split(\":\", 1)\n",
    "                except Exception: \n",
    "                    continue\n",
    "                print(f\"‚è≥ Testing {provider}/{model_id}...\")\n",
    "                try:\n",
    "                    out = connector.invoke(provider, model_id, messages, \n",
    "                                         temperature=1.0, request_timeout=180) \n",
    "                    text = out.get(\"text\", \"\")[:200].replace(\"\\n\", \" \")\n",
    "                    results.append([provider, model_id, \"‚úÖ\", text])\n",
    "                except Exception as e:\n",
    "                    results.append([provider, model_id, \"‚ùå\", str(e)])\n",
    "            headers = [\"Provider\", \"Model\", \"Status\", \"Response (truncated)\"]\n",
    "            print(\"\\nüìä Benchmark Results:\\n\")\n",
    "            display(Markdown(tabulate(results, headers=headers, tablefmt=\"github\"))) \n",
    "    \n",
    "    benchmark_invoke_button.on_click(on_invoke_benchmark_clicked)\n",
    "    tab_benchmark = widgets.VBox([\n",
    "        benchmark_model_selector, benchmark_prompt_input, \n",
    "        benchmark_invoke_button, benchmark_output_area\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 4: VISION TESTER\n",
    "    # =================================================================\n",
    "\n",
    "    vision_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"vision\"], \n",
    "        value=MODELS_BY_MODALITY[\"vision\"][0], \n",
    "        description='Vision Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    vision_url_input = widgets.Text(\n",
    "        value='https://ia-agence.ai/wp-content/uploads/2025/12/demo_image_ideal_ai_connector.jpg', \n",
    "        description='Image URL:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "    vision_prompt_input = widgets.Textarea(\n",
    "        value='Describe what you see in this image in detail.', \n",
    "        description='Prompt:', style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='90%', height='80px')\n",
    "    )\n",
    "    vision_invoke_button = widgets.Button(\n",
    "        description=\"üöÄ Analyze Image\", button_style='info', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    vision_image_area = widgets.Output(layout=Layout(width='400px', margin='10px'))\n",
    "    vision_text_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "    \n",
    "    def on_invoke_vision_clicked(b):\n",
    "        with vision_image_area: \n",
    "            clear_output(wait=True)\n",
    "        with vision_text_area: \n",
    "            clear_output(wait=True)\n",
    "        selected_pair = vision_llm_selector.value\n",
    "        url = vision_url_input.value\n",
    "        prompt = vision_prompt_input.value\n",
    "        try: \n",
    "            provider, model_id = selected_pair.split(\":\", 1)\n",
    "        except ValueError:\n",
    "            with vision_text_area: \n",
    "                print(f\"‚ùå Format error: {selected_pair}\")\n",
    "            return\n",
    "        try:\n",
    "            with vision_text_area: \n",
    "                print(f\"Downloading from {url}...\")\n",
    "            response = requests.get(url, timeout=20)\n",
    "            response.raise_for_status()\n",
    "            image_data_bytes = response.content\n",
    "            with vision_text_area: \n",
    "                print(\"Image downloaded.\")\n",
    "            with vision_image_area: \n",
    "                display(DisplayImage(data=image_data_bytes, width=400))\n",
    "            image_pil_object = PILImage.open(io.BytesIO(image_data_bytes)) if PILImage else None\n",
    "            input_for_connector = image_data_bytes\n",
    "            if image_pil_object and provider == \"google\":\n",
    "                input_for_connector = image_pil_object\n",
    "            with vision_text_area: \n",
    "                print(f\"Calling {provider}:{model_id}...\")\n",
    "            out = connector.invoke_image(\n",
    "                provider=provider, model_id=model_id, \n",
    "                image_input=input_for_connector, prompt=prompt, debug=False\n",
    "            )\n",
    "            with vision_text_area: \n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(f\"### ‚úÖ Response from **{model_id}**\\n---\\n{out.get('text')}\"))\n",
    "        except Exception as e:\n",
    "            with vision_text_area: \n",
    "                clear_output(wait=True)\n",
    "                display(Markdown(f\"### ‚ùå Vision Error\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "    \n",
    "    vision_invoke_button.on_click(on_invoke_vision_clicked)\n",
    "    tab_vision = widgets.VBox([\n",
    "        vision_llm_selector, vision_url_input, vision_prompt_input, \n",
    "        vision_invoke_button, widgets.HBox([vision_image_area, vision_text_area])\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 5: IMAGE GENERATION\n",
    "    # =================================================================\n",
    "    \n",
    "    image_gen_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"image_gen\"], \n",
    "        value=MODELS_BY_MODALITY[\"image_gen\"][0], \n",
    "        description='Image Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    image_gen_prompt_input = widgets.Textarea(\n",
    "        value='A Disney-style robot sitting on a rock, looking at a distant castle, warm sunset light.', \n",
    "        description='Prompt:', style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='90%', height='100px')\n",
    "    )\n",
    "    image_gen_invoke_button = widgets.Button(\n",
    "        description=\"üöÄ Generate Image\", button_style='warning', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    image_gen_display_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "    \n",
    "    def on_invoke_image_gen_clicked(b):\n",
    "        with image_gen_display_area:\n",
    "            clear_output(wait=True)\n",
    "            selected_pair = image_gen_llm_selector.value\n",
    "            prompt = image_gen_prompt_input.value\n",
    "            try: \n",
    "                provider, model_id = selected_pair.split(\":\", 1)\n",
    "            except ValueError: \n",
    "                print(f\"‚ùå Format error: {selected_pair}\")\n",
    "                return\n",
    "            print(f\"üé® Starting generation via {provider} with {model_id}...\")\n",
    "            try:\n",
    "                result = connector.invoke_image_generation(\n",
    "                    provider=provider, model_id=model_id, prompt=prompt, debug=False\n",
    "                )\n",
    "                if result and result.get(\"images\"):\n",
    "                    b64_str = result[\"images\"][0]\n",
    "                    img_bytes = base64.b64decode(b64_str)\n",
    "                    print(\"üñºÔ∏è Displaying image...\")\n",
    "                    display(DisplayImage(data=img_bytes))\n",
    "                    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "                    safe_model_id = model_id.replace(\":\", \"_\")\n",
    "                    filename = f\"{timestamp}_{provider}_{safe_model_id}.png\"\n",
    "                    final_output_path = OUTPUT_DIRS[\"image\"] / filename\n",
    "                    img_pil = PILImage.open(io.BytesIO(img_bytes))\n",
    "                    img_pil.save(final_output_path)\n",
    "                    print(f\"‚úÖ Image saved to: {final_output_path}\")\n",
    "                else: \n",
    "                    print(f\"‚ùå No image generated. Raw: {result.get('raw')}\")\n",
    "            except Exception as e: \n",
    "                display(Markdown(f\"### ‚ùå Image Generation Error\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "    \n",
    "    image_gen_invoke_button.on_click(on_invoke_image_gen_clicked)\n",
    "    tab_image_gen = widgets.VBox([\n",
    "        image_gen_llm_selector, image_gen_prompt_input, \n",
    "        image_gen_invoke_button, image_gen_display_area\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 6: VIDEO GENERATION\n",
    "    # =================================================================\n",
    "    \n",
    "    video_gen_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"video_gen\"], \n",
    "        value=MODELS_BY_MODALITY[\"video_gen\"][0], \n",
    "        description='Video Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    video_gen_prompt_input = widgets.Textarea(\n",
    "        value='A sleek futuristic robot standing in a high-tech urban environment at night.', \n",
    "        description='Prompt:', style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='90%', height='100px')\n",
    "    )\n",
    "    video_gen_invoke_button = widgets.Button(\n",
    "        description='üöÄ Generate Video (slow)', button_style='danger', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    video_gen_display_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "    \n",
    "    def on_invoke_video_gen_clicked(b):\n",
    "        with video_gen_display_area:\n",
    "            clear_output(wait=True)\n",
    "            selected_pair = video_gen_llm_selector.value\n",
    "            prompt = video_gen_prompt_input.value\n",
    "            try: \n",
    "                provider, model_id = selected_pair.split(\":\", 1)\n",
    "            except ValueError: \n",
    "                print(f\"‚ùå Format error: {selected_pair}\")\n",
    "                return\n",
    "            print(f\"üé¨ Starting video generation via {provider} with {model_id}...\")\n",
    "            print(\"‚è≥ This may take 1-5 minutes...\")\n",
    "            try:\n",
    "                result = connector.invoke_video_generation(\n",
    "                    provider=provider, model_id=model_id, prompt=prompt, debug=False\n",
    "                )\n",
    "                if result and result.get(\"videos\"):\n",
    "                    video_url = result[\"videos\"][0]\n",
    "                    print(f\"üéâ Video generated! URL: {video_url}\")\n",
    "                    print(\"‚¨áÔ∏è Downloading...\")\n",
    "                    video_bytes = requests.get(video_url, timeout=60).content\n",
    "                    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "                    safe_model_id = model_id.replace(\":\", \"_\")\n",
    "                    filename = f\"{timestamp}_{provider}_{safe_model_id}.mp4\"\n",
    "                    final_output_path = OUTPUT_DIRS[\"video\"] / filename\n",
    "                    with open(final_output_path, \"wb\") as f: \n",
    "                        f.write(video_bytes)\n",
    "                    print(f\"‚úÖ Video saved to: {final_output_path}\")\n",
    "                    print(\"\\nüé¨ Displaying video:\")\n",
    "                    display(Video(final_output_path, embed=True, width=400))\n",
    "                else: \n",
    "                    print(f\"‚ùå No video generated. Raw: {result.get('raw')}\")\n",
    "            except Exception as e: \n",
    "                display(Markdown(f\"### ‚ùå Video Generation Error\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "    \n",
    "    video_gen_invoke_button.on_click(on_invoke_video_gen_clicked)\n",
    "    tab_video_gen = widgets.VBox([\n",
    "        video_gen_llm_selector, video_gen_prompt_input, \n",
    "        video_gen_invoke_button, video_gen_display_area\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 7: SPEECH SYNTHESIS (TTS)\n",
    "    # =================================================================\n",
    "    \n",
    "    speech_gen_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"speech_gen\"], \n",
    "        value=MODELS_BY_MODALITY[\"speech_gen\"][0], \n",
    "        description='TTS Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    speech_gen_voice_selector = widgets.Dropdown(\n",
    "        options=['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'], \n",
    "        value='nova', description='Voice:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    speech_gen_prompt_input = widgets.Textarea(\n",
    "        value=\"Hello! This is a test of speech synthesis from the unified testing interface.\", \n",
    "        description='Text:', style={'description_width': 'auto'}, \n",
    "        layout=Layout(width='90%', height='100px')\n",
    "    )\n",
    "    speech_gen_invoke_button = widgets.Button(\n",
    "        description=\"üöÄ Generate Audio\", button_style='primary', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    speech_gen_display_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "    \n",
    "    def on_invoke_speech_gen_clicked(b):\n",
    "        with speech_gen_display_area:\n",
    "            clear_output(wait=True)\n",
    "            selected_pair = speech_gen_llm_selector.value\n",
    "            prompt = speech_gen_prompt_input.value\n",
    "            voice = speech_gen_voice_selector.value\n",
    "            try: \n",
    "                provider, model_id = selected_pair.split(\":\", 1)\n",
    "            except ValueError: \n",
    "                print(f\"‚ùå Format error: {selected_pair}\")\n",
    "                return\n",
    "            print(f\"üó£Ô∏è Starting speech synthesis via {provider} with {model_id} (Voice: {voice})...\")\n",
    "            try:\n",
    "                result = connector.invoke_speech_generation(\n",
    "                    provider=provider, model_id=model_id, \n",
    "                    text=prompt, voice=voice, debug=False\n",
    "                )\n",
    "                if result and result.get(\"audio_bytes\"):\n",
    "                    audio_bytes = result[\"audio_bytes\"]\n",
    "                    print(\"üéß Displaying audio player...\")\n",
    "                    display(Audio(data=audio_bytes))\n",
    "                    timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "                    safe_model_id = model_id.replace(\":\", \"_\")\n",
    "                    filename = f\"{timestamp}_{provider}_{safe_model_id}_{voice}.mp3\"\n",
    "                    final_output_path = OUTPUT_DIRS[\"speech\"] / filename\n",
    "                    with open(final_output_path, \"wb\") as f: \n",
    "                        f.write(audio_bytes)\n",
    "                    print(f\"‚úÖ Audio saved to: {final_output_path}\")\n",
    "                else: \n",
    "                    print(f\"‚ùå No audio generated. Raw: {result.get('raw')}\")\n",
    "            except Exception as e: \n",
    "                display(Markdown(f\"### ‚ùå Speech Synthesis Error\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "    \n",
    "    speech_gen_invoke_button.on_click(on_invoke_speech_gen_clicked)\n",
    "    tab_speech_gen = widgets.VBox([\n",
    "        speech_gen_llm_selector, speech_gen_voice_selector, \n",
    "        speech_gen_prompt_input, speech_gen_invoke_button, speech_gen_display_area\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 8: AUDIO TRANSCRIPTION (STT)\n",
    "    # =================================================================\n",
    "    \n",
    "    # 1. Automatic Audio File Management (Download from ia-agence.ai)\n",
    "    audio_filename = \"demo_sound_ideal_ai_connector.m4a\"\n",
    "    audio_url = \"https://ia-agence.ai/wp-content/uploads/2025/12/demo_sound_ideal_ai_connector.m4a\"\n",
    "    \n",
    "    if not os.path.exists(audio_filename):\n",
    "        print(f\"‚¨áÔ∏è Downloading demo audio from ia-agence.ai...\")\n",
    "        try:\n",
    "            r = requests.get(audio_url, timeout=10)\n",
    "            r.raise_for_status() \n",
    "            with open(audio_filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            print(\"‚úÖ Audio file downloaded successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to download audio demo: {e}\")\n",
    "            # Ensure the variable exists even if download fails\n",
    "            if not os.path.exists(audio_filename):\n",
    "                audio_filename = \"demo_sound_ideal_ai_connector.m4a\" \n",
    "\n",
    "    # 2. Widgets\n",
    "    audio_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"audio\"], \n",
    "        value=MODELS_BY_MODALITY[\"audio\"][0] if MODELS_BY_MODALITY[\"audio\"] else None, \n",
    "        description='STT Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    audio_file_input = widgets.Text(\n",
    "        value=audio_filename,  # Points to the local downloaded file\n",
    "        description='Audio File Path:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='90%')\n",
    "    )\n",
    "    \n",
    "    audio_language_input = widgets.Text(\n",
    "        value='en', description='Language (ISO code):', \n",
    "        placeholder='e.g., en, fr, es...', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    audio_invoke_button = widgets.Button(\n",
    "        description=\"üöÄ Transcribe Audio\", button_style='info', \n",
    "        layout=Layout(margin='10px 0 0 55px')\n",
    "    )\n",
    "    \n",
    "    audio_output_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "\n",
    "    # 3. La logique ne s'ex√©cute que quand on CLIQUE\n",
    "    def on_invoke_audio_clicked(b):\n",
    "        with audio_output_area:\n",
    "            clear_output(wait=True)\n",
    "            selected_pair = audio_llm_selector.value\n",
    "            audio_path_str = audio_file_input.value\n",
    "            language = audio_language_input.value\n",
    "\n",
    "            # V√©rification simple\n",
    "            if not os.path.exists(audio_path_str):\n",
    "                print(f\"‚ö†Ô∏è Erreur : Fichier introuvable : {audio_path_str}\")\n",
    "                return\n",
    "\n",
    "            try: \n",
    "                provider, model_id = selected_pair.split(\":\", 1)\n",
    "                print(f\"üé§ Starting transcription via {provider} with {model_id}...\")\n",
    "                \n",
    "                result = connector.invoke_audio(\n",
    "                    provider=provider, model_id=model_id, \n",
    "                    audio_file_path=audio_path_str, language=language, debug=False\n",
    "                )\n",
    "                \n",
    "                text = result.get(\"text\", \"Pas de texte retourn√©.\")\n",
    "                # Nettoyage si c'est du JSON brut\n",
    "                if isinstance(text, str) and text.strip().startswith(\"{\") and \"text\" in text:\n",
    "                     text = json.loads(text).get(\"text\", text)\n",
    "\n",
    "                display(Markdown(f\"### ‚úÖ Transcription :\\n---\\n{text}\"))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erreur : {e}\")\n",
    "    \n",
    "    audio_invoke_button.on_click(on_invoke_audio_clicked)\n",
    "    \n",
    "    tab_audio = widgets.VBox([\n",
    "        audio_llm_selector, audio_file_input, audio_language_input, \n",
    "        audio_invoke_button, audio_output_area\n",
    "    ])\n",
    "\n",
    "    # =================================================================\n",
    "    # TAB 9: VOICE CHAT (Full Audio Pipeline)\n",
    "    # =================================================================\n",
    "\n",
    "    # --- AJOUT IMPORTANT : Param√®tres du micro ---\n",
    "    SAMPLE_RATE = 44100  # Qualit√© standard CD\n",
    "    CHANNELS = 1         # 1 pour Mono (suffisant pour la voix)\n",
    "    # ---------------------------------------------\n",
    "    \n",
    "    voice_chat_llm_selector = widgets.Dropdown(\n",
    "        options=MODELS_BY_MODALITY[\"text\"], \n",
    "        value='openai:gpt-4o', \n",
    "        description='LLM Model:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    voice_chat_tts_selector = widgets.Dropdown(\n",
    "        options=['alloy', 'echo', 'fable', 'onyx', 'nova', 'shimmer'], \n",
    "        value='nova', description='TTS Voice:', \n",
    "        style={'description_width': 'initial'}, \n",
    "        layout=Layout(width='50%')\n",
    "    )\n",
    "    voice_chat_duration_slider = widgets.IntSlider(\n",
    "        value=5, min=2, max=10, step=1, \n",
    "        description='Duration (sec):', \n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    voice_chat_invoke_button = widgets.Button(\n",
    "        description='üöÄ Press and Speak', \n",
    "        button_style='success', \n",
    "        layout=Layout(margin='10px 0 0 55px', width='80%', height='50px')\n",
    "    )\n",
    "    voice_chat_output_area = widgets.Output(layout=Layout(margin='10px'))\n",
    "\n",
    "    def on_invoke_voice_chat_clicked(b):\n",
    "        with voice_chat_output_area:\n",
    "            clear_output(wait=True)\n",
    "            llm_pair = voice_chat_llm_selector.value\n",
    "            tts_voice = voice_chat_tts_selector.value\n",
    "            duration = voice_chat_duration_slider.value\n",
    "            llm_provider, llm_model_id = llm_pair.split(\":\", 1)\n",
    "            audio_in_file = \"voice_input_unified.wav\" \n",
    "\n",
    "            try:\n",
    "                # Step 1: Record\n",
    "                print(f\"\\nüì¢ Recording STARTED. Speak for {duration} seconds...\")\n",
    "                recording = sd.rec(int(duration * SAMPLE_RATE), \n",
    "                                  samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='int16')\n",
    "                sd.wait() \n",
    "                write(audio_in_file, SAMPLE_RATE, recording)  \n",
    "                print(f\"‚úÖ Recording saved to: {audio_in_file}\")\n",
    "\n",
    "                # Step 2: Transcription\n",
    "                print(\"\\n‚è≥ Step 2: Transcription (STT)...\")\n",
    "                stt_result = connector.invoke_audio(\n",
    "                    provider=\"infomaniak\", model_id=\"whisper\", \n",
    "                    audio_file_path=audio_in_file, language=\"en\", debug=False\n",
    "                )\n",
    "                transcribed_text = stt_result.get(\"text\", \"\").strip()\n",
    "                if not transcribed_text:\n",
    "                    print(\"üõë Transcription failed. Stopping.\")\n",
    "                    return\n",
    "                print(f\"üìù Transcribed: \\\"{transcribed_text}\\\"\")\n",
    "\n",
    "                # Step 3: LLM\n",
    "                print(f\"\\n‚è≥ Step 3: LLM processing with {llm_model_id}...\")\n",
    "                messages = [{\"role\": \"user\", \"content\": transcribed_text}]\n",
    "                llm_output = connector.invoke(\n",
    "                    provider=llm_provider, model_id=llm_model_id, \n",
    "                    messages=messages, temperature=0.7, debug=False\n",
    "                )\n",
    "                response_text = llm_output.get(\"text\", \"Sorry, I couldn't respond.\")\n",
    "                print(f\"ü§ñ LLM Response: \\\"{response_text}\\\"\")\n",
    "\n",
    "                # Step 4: TTS\n",
    "                print(\"\\n‚è≥ Step 4: Speech synthesis (TTS)...\")\n",
    "                tts_result = connector.invoke_speech_generation(\n",
    "                    provider=\"openai\", model_id=\"tts-1\", \n",
    "                    text=response_text, voice=tts_voice, \n",
    "                    response_format=\"mp3\", debug=False\n",
    "                )\n",
    "                audio_bytes = tts_result.get(\"audio_bytes\")\n",
    "                if not audio_bytes:\n",
    "                    print(\"‚ùå Speech synthesis failed.\")\n",
    "                    return\n",
    "\n",
    "                # Step 5: Save and play\n",
    "                timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "                safe_model_id = llm_model_id.replace(\":\", \"_\")\n",
    "                filename = f\"{timestamp}_voicechat_{safe_model_id}_{tts_voice}.mp3\"\n",
    "                final_output_path = OUTPUT_DIRS[\"speech\"] / filename\n",
    "                with open(final_output_path, \"wb\") as f:\n",
    "                    f.write(audio_bytes)\n",
    "                print(f\"‚úÖ Response audio saved to: {final_output_path}\")\n",
    "                print(f\"üéß Playing response...\")\n",
    "\n",
    "                IN_COLAB = 'google.colab' in sys.modules\n",
    "                if IN_COLAB:\n",
    "                    display(Audio(data=audio_bytes))\n",
    "                else:\n",
    "                    if pygame.mixer.get_init():\n",
    "                        try:\n",
    "                            pygame.mixer.music.load(final_output_path)\n",
    "                            pygame.mixer.music.play()\n",
    "                            while pygame.mixer.music.get_busy():\n",
    "                                time.sleep(0.1)\n",
    "                            print(\"‚úÖ Playback completed.\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ùå Playback error: {e}\")\n",
    "                    else:\n",
    "                        print(\"‚ö†Ô∏è pygame.mixer not initialized.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                display(Markdown(f\"### ‚ùå Voice Chat Error\"))\n",
    "                print(f\"Detail: {e}\")\n",
    "    \n",
    "    voice_chat_invoke_button.on_click(on_invoke_voice_chat_clicked)\n",
    "    \n",
    "    # Colab warning\n",
    "    IN_COLAB = 'google.colab' in sys.modules\n",
    "    voice_chat_widgets_list = [\n",
    "        widgets.HTML(\"<h3>üó£Ô∏èüéôÔ∏è Voice Chat (Full Audio Pipeline)</h3>\"\n",
    "                    \"<p>Speak, AI transcribes, processes, and responds vocally.</p>\"),\n",
    "    ]\n",
    "\n",
    "    if IN_COLAB:\n",
    "        colab_warning = widgets.HTML(\n",
    "            value=\"\"\"<div style='background-color: #fffbe6; border: 1px solid #ffe58f; \n",
    "                    padding: 10px; margin: 10px 0; border-radius: 4px;'>\n",
    "            <b>‚ö†Ô∏è Feature Unavailable on Google Colab</b>\n",
    "            <p style='margin: 5px 0 0 0;'>\n",
    "            Audio recording (via <code>sounddevice</code>) cannot access your microphone\n",
    "            from Colab's server.\n",
    "            </p>\n",
    "            <p style='margin: 5px 0 0 0;'>\n",
    "            <b>To use voice chat:</b> run this notebook locally (e.g., in VS Code).\n",
    "            </p>\n",
    "            </div>\"\"\"\n",
    "        )\n",
    "        voice_chat_widgets_list.append(colab_warning)\n",
    "        voice_chat_llm_selector.disabled = True\n",
    "        voice_chat_tts_selector.disabled = True\n",
    "        voice_chat_duration_slider.disabled = True\n",
    "        voice_chat_invoke_button.disabled = True\n",
    "        voice_chat_invoke_button.description = \"üöÄ (Unavailable on Colab)\"\n",
    "        voice_chat_invoke_button.button_style = ''\n",
    "\n",
    "    voice_chat_widgets_list.extend([\n",
    "        voice_chat_llm_selector, voice_chat_tts_selector, \n",
    "        voice_chat_duration_slider, voice_chat_invoke_button, voice_chat_output_area\n",
    "    ])\n",
    "    tab_voice_chat = widgets.VBox(voice_chat_widgets_list)\n",
    "\n",
    "    # =================================================================\n",
    "    # FINAL ASSEMBLY\n",
    "    # =================================================================\n",
    "\n",
    "    tab_widget = widgets.Tab(children=[\n",
    "        tab_text, tab_chat, tab_benchmark, tab_vision, \n",
    "        tab_image_gen, tab_video_gen, tab_speech_gen, tab_audio, tab_voice_chat\n",
    "    ])\n",
    "\n",
    "    # Set tab titles\n",
    "    tab_widget.set_title(0, f\"‚úçÔ∏è Text ({len(MODELS_BY_MODALITY['text'])} models)\")\n",
    "    tab_widget.set_title(1, f\"üí¨ Chat multi-turn\") \n",
    "    tab_widget.set_title(2, f\"üìä Benchmark\") \n",
    "    tab_widget.set_title(3, f\"üëÅÔ∏è Vision ({len(MODELS_BY_MODALITY['vision'])} models)\")\n",
    "    tab_widget.set_title(4, f\"üé® Image Gen ({len(MODELS_BY_MODALITY['image_gen'])} models)\")\n",
    "    tab_widget.set_title(5, f\"üé¨ Video Gen ({len(MODELS_BY_MODALITY['video_gen'])} models)\")\n",
    "    tab_widget.set_title(6, f\"üó£Ô∏è TTS ({len(MODELS_BY_MODALITY['speech_gen'])} models)\")\n",
    "    tab_widget.set_title(7, f\"üé§ STT ({len(MODELS_BY_MODALITY['audio'])} models)\") \n",
    "    tab_widget.set_title(8, f\"üó£Ô∏èüéôÔ∏è Voice Chat\") \n",
    "\n",
    "    # CSS styling\n",
    "    style = \"\"\"\n",
    "    <style>\n",
    "        .lm-TabBar-tab {\n",
    "            min-width: 130px !important; \n",
    "            max-width: 180px !important; \n",
    "            padding-left: 8px !important; \n",
    "            padding-right: 8px !important;\n",
    "            flex: 1 1 auto !important;   \n",
    "        }\n",
    "        .lm-TabBar-tabLabel {\n",
    "            white-space: normal !important; \n",
    "\n",
    "        }\n",
    "        .chat-output p {\n",
    "            font-size: 1em !important; \n",
    "            font-weight: normal !important; \n",
    "            margin: 0 !important;\n",
    "            padding: 0 !important;\n",
    "        }\n",
    "        .chat-output b {\n",
    "            font-weight: bold !important; \n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display interface\n",
    "    display(HTML(style))\n",
    "    display(Markdown(\"# ü§ñ Universal Testing Interface\"))\n",
    "    display(Markdown(\"*Quick-test all modalities with interactive widgets*\"))\n",
    "    display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Summary\n",
    "\n",
    "You've learned how to:\n",
    "\n",
    "‚úÖ Install and import `ideal-ai` as a proper package  \n",
    "‚úÖ Use text generation with multiple providers  \n",
    "‚úÖ Analyze images with vision models  \n",
    "‚úÖ Generate images from text  \n",
    "‚úÖ Transcribe audio to text  \n",
    "‚úÖ Synthesize speech from text  \n",
    "‚úÖ Generate videos (with async polling)  \n",
    "‚úÖ Add custom models at runtime  \n",
    "‚úÖ Integrate with smolagents  \n",
    "‚úÖ Debug API calls  \n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "1. **Extend config.json** - Add your favorite models without Python code\n",
    "2. **Build agents** - Combine multiple modalities in workflows\n",
    "3. **Deploy** - Use in production with proper error handling\n",
    "\n",
    "## üìñ Documentation\n",
    "\n",
    "- [GitHub Repository](https://github.com/Devgoodcode/ideal-ai)\n",
    "- [Connector API](https://github.com/Devgoodcode/ideal-ai/blob/main/src/ideal_ai/connector.py) - Full method signatures with docstrings\n",
    "- [Configuration Schema](https://github.com/Devgoodcode/ideal-ai/blob/main/src/ideal_ai/config.json) - Available families and models\n",
    "- [Examples](https://github.com/Devgoodcode/ideal-ai/tree/main/examples) - Working code samples\n",
    "\n",
    "## üì∫ See it in action\n",
    "\n",
    "[![Watch the Demo](https://img.youtube.com/vi/f1DwFRpo2HA/0.jpg)](https://www.youtube.com/watch?v=f1DwFRpo2HA)\n",
    "\n",
    "> *One Connector to Rule Them All. Watch the full demo (2.50 min).*\n",
    "\n",
    "## üë§ Author & Support\n",
    "\n",
    "**Gilles Blanchet**\n",
    "- üõ†Ô∏è Created by: [IA-Agence.ai](https://ia-agence.ai/ideal-ai-universal-llm-connector/) - *Need help integrating Generative AI? Let's talk.*\n",
    "- üåê Agency: [Idealcom.ch](https://idealcom.ch)\n",
    "- üêô GitHub: [@Devgoodcode](https://github.com/Devgoodcode)\n",
    "- üíº LinkedIn: [Gilles Blanchet](https://www.linkedin.com/in/gilles-blanchet-566ab759/)\n",
    "\n",
    "## üôè Acknowledgments\n",
    "\n",
    "This project is a labor of love, built on the shoulders of giants. Special thanks to:\n",
    "\n",
    "* **ü§ó Hugging Face**: For the fantastic *Agents Course*. It inspired me to create this connector to easily apply their concepts using my own existing tools (like Ollama & Infomaniak) without the hassle of writing wrappers.\n",
    "* **My AI Co-pilots & Mentors**:\n",
    "    * **Microsoft Copilot**: For the architectural breakthroughs (Families & Invoke concepts) and our late-night debates.\n",
    "    * **Perplexity**: For laying down the initial code foundation.\n",
    "    * **Google Gemini**: For the massive refactoring, patience, and pedagogical support in improving the core logic.\n",
    "    * **Kilo Code (Kimi & Claude)**: For the security testing, English translation, and PyPI publishing preparation.\n",
    "* **The Model Providers**: Ollama, Alibaba, Moonshot, MiniMax, OpenAI, and Infomaniak for their incredible technologies and platforms.\n",
    "* **The Open Source Community**: For the endless passion and knowledge sharing.\n",
    "\n",
    "Built with ‚ù§Ô∏è and passion, inspired by the open source AI community's need for a truly universal, maintainable LLM interface.\n",
    "\n",
    "*The adventure is just beginning...*\n",
    "\n",
    "---\n",
    "\n",
    "**One Connector to Rule Them All** üßô‚Äç‚ôÇÔ∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
